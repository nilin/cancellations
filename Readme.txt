#nilin
#2022/7

████████████████████████████████████████████████████████████████████████████████████████████████████

Examples

████████████████████████████████████████████████████████████████████████████████████████████████████


To compare the performance of ReLU and tanh, run example e2 or e3 with argument r/t

% python e2.py r
% python e2.py t

After running both a plot will be generated comparing the test losses of each (outputs/e2/comparetraining.pdf).


████████████████████████████████████████████████████████████████████████████████████████████████████

――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――
Force parameter updates with named definitions from command line
――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――

% python e1.py
% python e1.py n=6 samples=25000 widths=6,50,1


